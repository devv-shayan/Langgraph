# Understanding LangGraph and LLM Agents

## Introduction

I'm exploring LangGraph, a framework for building agent and multi-agent applications. What makes LangGraph interesting is its ability to help developers add better precision and control into agent workflows. This is particularly important when building systems that need to execute complex tasks reliably.

## Technical Setup

First, let's install the required packages:

```bash
pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python
```

### Setting up API Keys

We need to set up our API keys for OpenAI and Tavily:

```python
import os, getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

## Working with Chat Models

I'm using OpenAI's chat models for this exploration. They take sequences of messages as inputs and return chat messages as outputs. Here's how to set them up:

```python
from langchain_openai import ChatOpenAI
gpt4o_chat = ChatOpenAI(model="gpt-4o", temperature=0)
gpt35_chat = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
```

### Testing the Chat Models

Let's test out how these models work with different types of inputs:

```python
from langchain_core.messages import HumanMessage

# Create a message
msg = HumanMessage(content="Hello world", name="MyName")

# Message list
messages = [msg]

# Test both models
gpt4o_chat.invoke(messages)
gpt4o_chat.invoke("hello world")
gpt35_chat.invoke("hello world")
```

## Implementing Search Functionality

I'm using Tavily for search capabilities. It's optimized for LLMs and RAG applications:

```python
_set_env("TAVILY_API_KEY")

from langchain_community.tools.tavily_search import TavilySearchResults
tavily_search = TavilySearchResults(max_results=3)
search_docs = tavily_search.invoke("What is LangGraph?")

# View search results
print(search_docs)
```

## Key Points

- Temperature setting of 0 is used for more deterministic outputs
- Both GPT-4 and GPT-3.5 models are set up for comparison
- Tavily search is limited to 3 results for conciseness
- The chat models can take both direct strings and message objects as input

This setup provides a foundation for building more complex agent-based systems using LangGraph.

## Sample Output

When running a search query about LangGraph, you might get results like:

```python
[
    {
        'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',
        'content': 'LangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.'
    },
    {
        'url': 'https://langchain-ai.github.io/langgraph/',
        'content': 'Overview LangGraph is a library for building stateful, multi-actor applications with LLMs...'
    },
    {
        'url': 'https://www.youtube.com/watch?v=nmDFSVRnr4Q',
        'content': 'LangGraph is an extension of LangChain enabling Multi-Agent conversation and cyclic chains...'
    }
]
```

## Next Steps

- Experiment with different temperature settings
- Try different search queries
- Build more complex agent workflows
- Test different model combinations

Feel free to modify the code examples and experiment with different configurations to suit your specific needs.
