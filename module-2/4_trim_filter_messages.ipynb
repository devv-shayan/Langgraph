{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c52ea2f9-03ff-4647-b782-46867ebed04e",
      "metadata": {
        "id": "c52ea2f9-03ff-4647-b782-46867ebed04e"
      },
      "source": [
        "# Filtering and trimming messages\n",
        "\n",
        "## Review\n",
        "\n",
        "Now, we have a deeper understanding of a few things:\n",
        "\n",
        "* How to customize the graph state schema\n",
        "* How to define custom state reducers\n",
        "* How to use multiple graph state schemas\n",
        "\n",
        "## Goals\n",
        "\n",
        "Now, we can start using these concepts with models in LangGraph!\n",
        "\n",
        "In the next few sessions, we'll build towards a chatbot that has long-term memory.\n",
        "\n",
        "Because our chatbot will use messages, let's first talk a bit more about advanced ways to work with messages in graph state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda",
      "metadata": {
        "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain langchain_core langgraph langchain_google_genai python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768dc606-d5f2-468d-96ea-910b264e0f8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "768dc606-d5f2-468d-96ea-910b264e0f8a",
        "outputId": "2d063afc-8fd9-43b0-cbb1-25e1df82ad3e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, dotenv_values\n",
        "\n",
        "# Load environment variables from .env.example\n",
        "load_dotenv(\"../.env.example\")\n",
        "\n",
        "def debug_api_key(key_name):\n",
        "    print(f\"\\nDebugging {key_name}:\")\n",
        "    \n",
        "    # Check environment variable\n",
        "    env_value = os.getenv(key_name)\n",
        "    print(f\"1. Value from os.getenv('{key_name}'): {env_value}\")\n",
        "    \n",
        "    # Check .env.example file directly\n",
        "    config = dotenv_values(\"../.env.example\")\n",
        "    dotenv_value = config.get(key_name)\n",
        "    print(f\"2. Value from .env.example: {dotenv_value}\")\n",
        "    \n",
        "    # Read .env.example file manually\n",
        "    try:\n",
        "        with open(\"../.env.example\", 'r') as f:\n",
        "            content = f.read()\n",
        "            print(f\"3. Content of .env.example:\")\n",
        "            print(content)\n",
        "    except FileNotFoundError:\n",
        "        print(\"3. Error: .env.example file not found\")\n",
        "    \n",
        "    # Try to parse the value manually\n",
        "    if dotenv_value:\n",
        "        cleaned_value = dotenv_value.strip().strip(\"'\").strip('\"')\n",
        "        print(f\"4. Cleaned value: {cleaned_value}\")\n",
        "        \n",
        "        # Set the environment variable\n",
        "        os.environ[key_name] = cleaned_value\n",
        "        print(f\"5. Environment variable set. New value: {os.getenv(key_name)}\")\n",
        "    else:\n",
        "        print(\"4. Unable to parse value from .env.example\")\n",
        "\n",
        "# Debug both API keys\n",
        "debug_api_key('GOOGLE_API_KEY')\n",
        "debug_api_key('LANGCHAIN_API_KEY')\n",
        "\n",
        "print(\"\\nFinal environment variable values:\")\n",
        "print(f\"GOOGLE_API_KEY: {os.getenv('GOOGLE_API_KEY')}\")\n",
        "print(f\"LANGCHAIN_API_KEY: {os.getenv('LANGCHAIN_API_KEY')}\")\n",
        "\n",
        "print(\"\\nExample of how to use the API keys in your code:\")\n",
        "print(\"import os\")\n",
        "print(\"google_api_key = os.environ.get('GOOGLE_API_KEY')\")\n",
        "print(\"langchain_api_key = os.environ.get('LANGCHAIN_API_KEY')\")\n",
        "print(\"print(f'My Google API Key: {google_api_key}')\")\n",
        "print(\"print(f'My LangChain API Key: {langchain_api_key}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b64d8d3-e4ac-4961-bdc0-688825eb5864",
      "metadata": {
        "id": "8b64d8d3-e4ac-4961-bdc0-688825eb5864"
      },
      "source": [
        "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing).\n",
        "\n",
        "We'll log to a project, `langchain-academy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dd020c79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd020c79",
        "outputId": "ce100d1b-c3e8-4451-e3e0-7dbc3749131e"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get('GOOGLE_API_KEY')\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langraphlearning2.0\"\n",
        "client = Client()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "893fe6a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"LANGCHAIN_TRACING_V2: {os.getenv('LANGCHAIN_TRACING_V2')}\")\n",
        "print(f\"LANGCHAIN_PROJECT: {os.getenv('LANGCHAIN_PROJECT')}\")\n",
        "print({os.getenv(\"LANGCHAIN_API_KEY\")})\n",
        "print({os.getenv(\"LANGCHAIN_ENDPOINT\")})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72f3fc90-58b6-4f7f-897e-dddf6ae532c7",
      "metadata": {
        "id": "72f3fc90-58b6-4f7f-897e-dddf6ae532c7"
      },
      "source": [
        "## Messages as state\n",
        "\n",
        "First, let's define some messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf11a463-e27a-4a05-b41d-64882e38edca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf11a463-e27a-4a05-b41d-64882e38edca",
        "outputId": "b6655be9-9d89-4f1e-932a-f5197385a8a6"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "messages = [AIMessage(f\"So you said you were researching llms?\", name=\"AI\")]\n",
        "messages.append(HumanMessage(f\"Yes, I know about chatgpt 4o. But what others should I learn about?\", name=\"Shayan\"))\n",
        "\n",
        "for m in messages:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b814adcb-6bf9-4b75-be11-e59f933fbd0c",
      "metadata": {
        "id": "b814adcb-6bf9-4b75-be11-e59f933fbd0c"
      },
      "source": [
        "Recall we can pass them to a chat model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "051c4ec1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd1dab8-0af8-4621-8264-ce65065f76ec",
      "metadata": {
        "id": "fbd1dab8-0af8-4621-8264-ce65065f76ec"
      },
      "source": [
        "We can run our chat model in a simple graph with `MessagesState`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd8c39c-633b-4176-9cc6-8318e42bb5dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "bbd8c39c-633b-4176-9cc6-8318e42bb5dd",
        "outputId": "4b556a13-665b-49f2-86b3-d2db04601fe6"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Node\n",
        "def chat_model_node(state: MessagesState) -> MessagesState:\n",
        "    return {\"messages\": llm.invoke(state[\"messages\"])}\n",
        "\n",
        "# Build graph\n",
        "builder: StateGraph = StateGraph(MessagesState)\n",
        "builder.add_node(\"chat_model\", chat_model_node)\n",
        "builder.add_edge(START, \"chat_model\")\n",
        "builder.add_edge(\"chat_model\", END)\n",
        "graph: CompiledStateGraph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a5a3e4a-ccfd-4d14-81f1-f0de6e11a1e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a5a3e4a-ccfd-4d14-81f1-f0de6e11a1e4",
        "outputId": "94337b21-424e-481b-a115-3295b1b893ee"
      },
      "outputs": [],
      "source": [
        "output = graph.invoke({'messages': messages})\n",
        "for m in output['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Kl4lGlZyxej",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kl4lGlZyxej",
        "outputId": "ba627553-5f61-411b-d70c-98494367cc30"
      },
      "outputs": [],
      "source": [
        "async for m in graph.astream_events({'messages': messages}, version=\"v2\"): # The version argument is now correctly passed as a keyword argument.\n",
        "      print(m)\n",
        "      print(\"\\n--------------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c33e63-1ef4-412d-bb10-6a1b9e5b35a7",
      "metadata": {
        "id": "34c33e63-1ef4-412d-bb10-6a1b9e5b35a7"
      },
      "source": [
        "## Reducer\n",
        "\n",
        "A practical challenge when working with messages is managing long-running conversations.\n",
        "\n",
        "Long-running conversations result in high token usage and latency if we are not careful, because we pass a growing list of messages to the model.\n",
        "\n",
        "We have a few ways to address this.\n",
        "\n",
        "First, recall the trick we saw using `RemoveMessage` and the `add_messages` reducer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222c6bc5-bb0e-4a43-80f5-c8ec38d99f3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "222c6bc5-bb0e-4a43-80f5-c8ec38d99f3a",
        "outputId": "fb6b40c3-70d0-46e8-9d74-3389837cafbd"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import RemoveMessage\n",
        "\n",
        "# Nodes\n",
        "def filter_messages(state: MessagesState) -> MessagesState:\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"messages\": delete_messages}\n",
        "\n",
        "def chat_model_node(state: MessagesState) -> MessagesState:\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Build graph\n",
        "builder: StateGraph = StateGraph(MessagesState)\n",
        "builder.add_node(\"filter\", filter_messages)\n",
        "builder.add_node(\"chat_model\", chat_model_node)\n",
        "builder.add_edge(START, \"filter\")\n",
        "builder.add_edge(\"filter\", \"chat_model\")\n",
        "builder.add_edge(\"chat_model\", END)\n",
        "graph: CompiledStateGraph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a7c2cc-54ce-43e7-9a90-abf37827d709",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a7c2cc-54ce-43e7-9a90-abf37827d709",
        "outputId": "0324a3ed-1934-45db-f71b-725662ba5f9b"
      },
      "outputs": [],
      "source": [
        "# Message list with a preamble\n",
        "messages = [AIMessage(\"Hi.\", name=\"AI\", id=\"1\")]\n",
        "messages.append(HumanMessage(\"Hi.\", name=\"Shayan\", id=\"2\"))\n",
        "messages.append(AIMessage(\"So you said you were researching about llms?\", name=\"AI\", id=\"3\"))\n",
        "messages.append(HumanMessage(\"Yes, I know about llms. But what others should I learn about?\", name=\"Shayan\", id=\"4\"))\n",
        "\n",
        "# Invoke\n",
        "output = graph.invoke({'messages': messages})\n",
        "for m in output['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f506457d-014b-4fee-a684-e5edfb4b8f0d",
      "metadata": {
        "id": "f506457d-014b-4fee-a684-e5edfb4b8f0d"
      },
      "source": [
        "## Filtering messages\n",
        "\n",
        "If you don't need or want to modify the graph state, you can just filter the messages you pass to the chat model.\n",
        "\n",
        "For example, just pass in a filtered list: `llm.invoke(messages[-1:])` to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d0b904-7cd6-486b-8948-105bee3d4683",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "22d0b904-7cd6-486b-8948-105bee3d4683",
        "outputId": "f429d516-5ace-4986-bbe6-1b842fd3ebf4"
      },
      "outputs": [],
      "source": [
        "# Node\n",
        "def chat_model_node(state: MessagesState):\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"][-1:])]}\n",
        "\n",
        "# Build graph\n",
        "builder: StateGraph = StateGraph(MessagesState)\n",
        "builder.add_node(\"chat_model\", chat_model_node)\n",
        "builder.add_edge(START, \"chat_model\")\n",
        "builder.add_edge(\"chat_model\", END)\n",
        "graph: CompiledStateGraph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f58c6fc-532f-418d-b70a-cfcb3307daf5",
      "metadata": {
        "id": "6f58c6fc-532f-418d-b70a-cfcb3307daf5"
      },
      "source": [
        "Let's take our existing list of messages, append the above LLM response, and append a follow-up question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "16956015-1dbe-4108-89b5-4209b68b51ca",
      "metadata": {
        "id": "16956015-1dbe-4108-89b5-4209b68b51ca"
      },
      "outputs": [],
      "source": [
        "messages.append(output['messages'][-1])\n",
        "messages.append(HumanMessage(f\"Tell me more about llms!\", name=\"Shayan\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85563415-c085-46a8-a4ac-155df798c54e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85563415-c085-46a8-a4ac-155df798c54e",
        "outputId": "7e3dffce-bda8-4e12-fa63-545bc0f5cd1e"
      },
      "outputs": [],
      "source": [
        "for m in messages:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23349705-a059-47b5-9760-d8f64e687393",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23349705-a059-47b5-9760-d8f64e687393",
        "outputId": "9065d0c3-08e7-47f3-a695-0d56b16bf34d"
      },
      "outputs": [],
      "source": [
        "# Invoke, using message filtering\n",
        "output = graph.invoke({'messages': messages})\n",
        "for m in output['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42e1d8d2-e297-4d78-b54c-d12b3c866745",
      "metadata": {
        "id": "42e1d8d2-e297-4d78-b54c-d12b3c866745"
      },
      "source": [
        "The state has all of the mesages.\n",
        "\n",
        "But, let's look at the LangSmith trace to see that the model invocation only uses the last message:\n",
        "\n",
        "https://smith.langchain.com/public/75aca3ce-ef19-4b92-94be-0178c7a660d9/r"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc40d930-3c1f-47fe-8d2a-ce174873353c",
      "metadata": {
        "id": "fc40d930-3c1f-47fe-8d2a-ce174873353c"
      },
      "source": [
        "## Trim messages\n",
        "\n",
        "Another approach is to [trim messages](https://python.langchain.com/v0.2/docs/how_to/trim_messages/#getting-the-last-max_tokens-tokens), based upon a set number of tokens.\n",
        "\n",
        "This restricts the message history to a specified number of tokens.\n",
        "\n",
        "While filtering only returns a post-hoc subset of the messages between agents, trimming restricts the number of tokens that a chat model can use to respond.\n",
        "\n",
        "See the `trim_messages` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff99b81-cf03-4cc2-b44f-44829a73e1fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "2ff99b81-cf03-4cc2-b44f-44829a73e1fd",
        "outputId": "dd032e1c-87f5-47a8-f9d4-dbe4541441a6"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import trim_messages\n",
        "\n",
        "# Node\n",
        "def chat_model_node(state: MessagesState):\n",
        "    messages = trim_messages(\n",
        "            state[\"messages\"],\n",
        "            max_tokens=100,\n",
        "            strategy=\"last\",\n",
        "            token_counter=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\"),\n",
        "            allow_partial=True,\n",
        "        )\n",
        "    return {\"messages\": [llm.invoke(messages)]}\n",
        "\n",
        "# Build graph\n",
        "builder = StateGraph(MessagesState)\n",
        "builder.add_node(\"chat_model\", chat_model_node)\n",
        "builder.add_edge(START, \"chat_model\")\n",
        "builder.add_edge(\"chat_model\", END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "24df63ac-da29-4874-b3df-7e390e97cc8a",
      "metadata": {
        "id": "24df63ac-da29-4874-b3df-7e390e97cc8a"
      },
      "outputs": [],
      "source": [
        "messages.append(output['messages'][-1])\n",
        "messages.append(HumanMessage(f\"Tell me in AI domain where google beats openai !\", name=\"Shayan\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9d8971-c75c-43ca-a209-eb1d07b2ead0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d9d8971-c75c-43ca-a209-eb1d07b2ead0",
        "outputId": "196a7e29-9082-4eff-b452-614000c4a333"
      },
      "outputs": [],
      "source": [
        "# Example of trimming messages\n",
        "trim_messages(\n",
        "            messages,\n",
        "            max_tokens=100,\n",
        "            strategy=\"last\",\n",
        "            token_counter=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\"),\n",
        "            allow_partial=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed70a269-a869-4fa0-a1df-29736a432c51",
      "metadata": {
        "id": "ed70a269-a869-4fa0-a1df-29736a432c51"
      },
      "outputs": [],
      "source": [
        "# Invoke, using message trimming in the chat_model_node\n",
        "messages_out_trim = graph.invoke({'messages': messages})\n",
        "for m in output['messages']:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b3db67-380e-46b5-9a6a-20100ba52008",
      "metadata": {
        "id": "38b3db67-380e-46b5-9a6a-20100ba52008"
      },
      "source": [
        "Let's look at the LangSmith trace to see the model invocation:\n",
        "\n",
        "https://smith.langchain.com/public/b153f7e9-f1a5-4d60-8074-f0d7ab5b42ef/r"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
