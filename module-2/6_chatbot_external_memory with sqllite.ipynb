{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "af6c7afe-1037-41ab-98e4-494692e47402",
      "metadata": {
        "id": "af6c7afe-1037-41ab-98e4-494692e47402"
      },
      "source": [
        "# Chatbot with message summarization & external DB memory\n",
        "\n",
        "## Review\n",
        "\n",
        "We've covered how to customize graph state schema and reducer.\n",
        "\n",
        "We've also shown a number of tricks for trimming or filtering messages in graph state.\n",
        "\n",
        "We've used these concepts in a Chatbot with memory that produces a running summary of the conversation.\n",
        "\n",
        "## Goals\n",
        "\n",
        "But, what if we want our Chatbot to have memory that persists indefinitely?\n",
        "\n",
        "Now, we'll introduce some more advanced checkpointers that support external databases.\n",
        "\n",
        "Here, we'll show how to use [Sqlite as a checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer), but other checkpointers, such as [Postgres](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/) are available!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "85ed78d9-6ca2-45ac-96a9-52e341ec519d",
      "metadata": {
        "id": "85ed78d9-6ca2-45ac-96a9-52e341ec519d"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph-checkpoint-sqlite langchain_core langgraph langchain_google_genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e10c4d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e10c4d4",
        "outputId": "a400e97e-05a7-4ef8-f34b-a6ddfba84cc0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, dotenv_values\n",
        "\n",
        "# Load environment variables from .env.example\n",
        "load_dotenv(\"../.env.example\")\n",
        "\n",
        "def debug_api_key(key_name):\n",
        "    print(f\"\\nDebugging {key_name}:\")\n",
        "    \n",
        "    # Check environment variable\n",
        "    env_value = os.getenv(key_name)\n",
        "    print(f\"1. Value from os.getenv('{key_name}'): {env_value}\")\n",
        "    \n",
        "    # Check .env.example file directly\n",
        "    config = dotenv_values(\"../.env.example\")\n",
        "    dotenv_value = config.get(key_name)\n",
        "    print(f\"2. Value from .env.example: {dotenv_value}\")\n",
        "    \n",
        "    # Read .env.example file manually\n",
        "    try:\n",
        "        with open(\"../.env.example\", 'r') as f:\n",
        "            content = f.read()\n",
        "            print(f\"3. Content of .env.example:\")\n",
        "            print(content)\n",
        "    except FileNotFoundError:\n",
        "        print(\"3. Error: .env.example file not found\")\n",
        "    \n",
        "    # Try to parse the value manually\n",
        "    if dotenv_value:\n",
        "        cleaned_value = dotenv_value.strip().strip(\"'\").strip('\"')\n",
        "        print(f\"4. Cleaned value: {cleaned_value}\")\n",
        "        \n",
        "        # Set the environment variable\n",
        "        os.environ[key_name] = cleaned_value\n",
        "        print(f\"5. Environment variable set. New value: {os.getenv(key_name)}\")\n",
        "    else:\n",
        "        print(\"4. Unable to parse value from .env.example\")\n",
        "\n",
        "# Debug both API keys\n",
        "debug_api_key('GOOGLE_API_KEY')\n",
        "debug_api_key('LANGCHAIN_API_KEY')\n",
        "\n",
        "print(\"\\nFinal environment variable values:\")\n",
        "print(f\"GOOGLE_API_KEY: {os.getenv('GOOGLE_API_KEY')}\")\n",
        "print(f\"LANGCHAIN_API_KEY: {os.getenv('LANGCHAIN_API_KEY')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6a002315",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a002315",
        "outputId": "802d4e6d-cd6e-4995-f2d8-de89ae89cb35"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get('GOOGLE_API_KEY')\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langraphlearning2.1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b40d25c0-e9b5-4854-bf07-3cc3ff07122e",
      "metadata": {
        "id": "b40d25c0-e9b5-4854-bf07-3cc3ff07122e"
      },
      "source": [
        "## Sqlite\n",
        "\n",
        "A good starting point here is the [SqliteSaver checkpointer](https://langchain-ai.github.io/langgraph/concepts/low_level/#checkpointer).\n",
        "\n",
        "Sqlite is a [small, fast, highly popular](https://x.com/karpathy/status/1819490455664685297) SQL database.\n",
        "\n",
        "If we supply `\":memory:\"` it creates an in-memory Sqlite database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fae15402-17ae-4e89-8ecf-4c89e08b22fe",
      "metadata": {
        "id": "fae15402-17ae-4e89-8ecf-4c89e08b22fe"
      },
      "outputs": [],
      "source": [
        "import sqlite3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2bf53ec-6d4a-42ce-8183-344795eed403",
      "metadata": {
        "id": "c2bf53ec-6d4a-42ce-8183-344795eed403"
      },
      "source": [
        "But, if we supply a db path, then it will create a database for us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "58339167-920c-4994-a0a7-0a9c5d4f7cf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58339167-920c-4994-a0a7-0a9c5d4f7cf7",
        "outputId": "6ec6961d-9c61-4cfd-80e7-3ba5ee6ab247"
      },
      "outputs": [],
      "source": [
        "db_path = \"./6_chatbot_external_memory.db\"\n",
        "conn = sqlite3.connect(db_path, check_same_thread=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3c7736b6-a750-48f8-a838-8e7616b12250",
      "metadata": {
        "id": "3c7736b6-a750-48f8-a838-8e7616b12250"
      },
      "outputs": [],
      "source": [
        "# Here is our checkpointer\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "memory: SqliteSaver = SqliteSaver(conn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d8cb629-213f-4b87-965e-19b812c42da1",
      "metadata": {
        "id": "9d8cb629-213f-4b87-965e-19b812c42da1"
      },
      "source": [
        "Let's re-define our chatbot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc414e29-2078-41a0-887c-af1a6a3d72c0",
      "metadata": {
        "id": "dc414e29-2078-41a0-887c-af1a6a3d72c0"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "\n",
        "from langgraph.graph import END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(model = \"gemini-1.5-flash\")\n",
        "\n",
        "class State(MessagesState):\n",
        "    summary: str\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State) -> State:\n",
        "\n",
        "    # Get summary if it exists\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State) -> State:\n",
        "\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date and answer it accordingly: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above and answer it accordingly:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
        "\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State) -> State:\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c13c0b-a383-4f73-9cc1-63f0eed8f190",
      "metadata": {
        "id": "41c13c0b-a383-4f73-9cc1-63f0eed8f190"
      },
      "source": [
        "Now, we just re-compile with our sqlite checkpointer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e867fd95-91eb-4ce1-82fc-bb72d611a96d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "e867fd95-91eb-4ce1-82fc-bb72d611a96d",
        "outputId": "2ca033b1-00c7-47e5-af2a-e91c786e2f5a"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Define a new graph\n",
        "workflow: StateGraph = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(summarize_conversation)\n",
        "\n",
        "# Set the entrypoint as conversation\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# Compile\n",
        "graph: CompiledStateGraph = workflow.compile(checkpointer=memory)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8769db99-3938-45e6-a594-56beb18d6c45",
      "metadata": {
        "id": "8769db99-3938-45e6-a594-56beb18d6c45"
      },
      "source": [
        "Now, we can invoke the graph several times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4094a0-d240-4be8-903a-7d9f605bdc5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f4094a0-d240-4be8-903a-7d9f605bdc5c",
        "outputId": "9ab4170f-6563-4fff-8533-82b32125c2a6"
      },
      "outputs": [],
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# Start conversation\n",
        "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "input_message = HumanMessage(content=\"what's my name?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()\n",
        "\n",
        "input_message = HumanMessage(content=\"i like the 49ers!\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3666b40d",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_message = HumanMessage(content=\"whats my dogs name?\")\n",
        "output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "for m in output['messages'][-1:]:\n",
        "    m.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f3e842-4497-45e2-a924-69672a9bcb33",
      "metadata": {
        "id": "c0f3e842-4497-45e2-a924-69672a9bcb33"
      },
      "source": [
        "Let's confirm that our state is saved locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2ab158a-5a82-417a-8841-730a4cc18ea7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2ab158a-5a82-417a-8841-730a4cc18ea7",
        "outputId": "4c173155-1d7d-4fae-8757-9c833eeab4ab"
      },
      "outputs": [],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e21152d-ed9c-408d-b7d5-f634c9ce81e2",
      "metadata": {
        "id": "1e21152d-ed9c-408d-b7d5-f634c9ce81e2"
      },
      "source": [
        "### Persisting state\n",
        "\n",
        "Using database like Sqlite means state is persisted!\n",
        "\n",
        "For example, we can re-start the notebook kernel and see that we can still load from Sqlite DB on disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a44dc5-be04-45fa-a6fc-27b0f8ee4678",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9a44dc5-be04-45fa-a6fc-27b0f8ee4678",
        "outputId": "887ade22-4304-4370-c654-d42e4183c068"
      },
      "outputs": [],
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "graph_state = graph.get_state(config)\n",
        "graph_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8466e418-1a46-4cdb-a51a-6ae14281bb85",
      "metadata": {
        "id": "8466e418-1a46-4cdb-a51a-6ae14281bb85"
      },
      "source": [
        "## LangGraph Studio\n",
        "\n",
        "--\n",
        "\n",
        "**⚠️ DISCLAIMER**\n",
        "\n",
        "*Running Studio currently requires a Mac. If you are not using a Mac, then skip this step.*\n",
        "\n",
        "--\n",
        "\n",
        "Now that we better understand external memory, recall that the LangGraph API packages your code and provides you with with built-in persistence.\n",
        "\n",
        "And the API is the back-end for Studio!\n",
        "\n",
        "Load the `chatbot` in the UI, which uses `module2-/studio/chatbot.py` set in `module2-/studio/langgraph.json`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4916d8b",
      "metadata": {
        "id": "c4916d8b"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
