{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet -U langgraph langgraph-checkpoint-postgres psycopg2-binary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "# Load environment variables from .env.example\n",
    "load_dotenv(\"../.env.example\")\n",
    "\n",
    "def debug_api_key(key_name):\n",
    "    print(f\"\\nDebugging {key_name}:\")\n",
    "    \n",
    "    # Check environment variable\n",
    "    env_value = os.getenv(key_name)\n",
    "    print(f\"1. Value from os.getenv('{key_name}'): {env_value}\")\n",
    "    \n",
    "    # Check .env.example file directly\n",
    "    config = dotenv_values(\"../.env.example\")\n",
    "    dotenv_value = config.get(key_name)\n",
    "    print(f\"2. Value from .env.example: {dotenv_value}\")\n",
    "    \n",
    "    # Read .env.example file manually\n",
    "    try:\n",
    "        with open(\"../.env.example\", 'r') as f:\n",
    "            content = f.read()\n",
    "            print(f\"3. Content of .env.example:\")\n",
    "            print(content)\n",
    "    except FileNotFoundError:\n",
    "        print(\"3. Error: .env.example file not found\")\n",
    "    \n",
    "    # Try to parse the value manually\n",
    "    if dotenv_value:\n",
    "        cleaned_value = dotenv_value.strip().strip(\"'\").strip('\"')\n",
    "        print(f\"4. Cleaned value: {cleaned_value}\")\n",
    "        \n",
    "        # Set the environment variable\n",
    "        os.environ[key_name] = cleaned_value\n",
    "        print(f\"5. Environment variable set. New value: {os.getenv(key_name)}\")\n",
    "    else:\n",
    "        print(\"4. Unable to parse value from .env.example\")\n",
    "\n",
    "# Debug both API keys\n",
    "debug_api_key('GOOGLE_API_KEY')\n",
    "debug_api_key('LANGCHAIN_API_KEY')\n",
    "\n",
    "print(\"\\nFinal environment variable values:\")\n",
    "print(f\"GOOGLE_API_KEY: {os.getenv('GOOGLE_API_KEY')}\")\n",
    "print(f\"LANGCHAIN_API_KEY: {os.getenv('LANGCHAIN_API_KEY')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "from psycopg2.pool import SimpleConnectionPool\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph.state import CompiledStateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST', '127.0.0.1'),\n",
    "    'port': int(os.getenv('POSTGRES_PORT', 5432)),\n",
    "    'dbname': os.getenv('POSTGRES_DB', 'postgres'),\n",
    "    'user': os.getenv('POSTGRES_USER', 'postgres'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', '123456'),\n",
    "    'application_name': 'langgraph_app',\n",
    "    'keepalives': 1,\n",
    "    'keepalives_idle': 30,\n",
    "    'keepalives_interval': 10,\n",
    "    'keepalives_count': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# State class definition\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Model calling function\n",
    "def call_model(state: State) -> State:\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Summarization function\n",
    "def summarize_conversation(state: State) -> State:\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date and answer it accordingly: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above and answer it accordingly:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Continue function\n",
    "def should_continue(state: State) -> str:\n",
    "    messages = state[\"messages\"]\n",
    "    return \"summarize_conversation\" if len(messages) > 6 else END\n",
    "\n",
    "# Create and compile the graph\n",
    "def create_workflow():\n",
    "    try:\n",
    "        workflow = StateGraph(State)\n",
    "        workflow.add_node(\"conversation\", call_model)\n",
    "        workflow.add_node(summarize_conversation)\n",
    "        workflow.add_edge(START, \"conversation\")\n",
    "        workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "        workflow.add_edge(\"summarize_conversation\", END)\n",
    "        \n",
    "        return workflow.compile(checkpointer=memory)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating workflow: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the graph\n",
    "graph = create_workflow()\n",
    "\n",
    "# Configuration for graph invocation\n",
    "def get_graph_config():\n",
    "    return {\n",
    "        \"connection\": db_manager.get_connection(),\n",
    "        \"configurable\": {\"thread_id\": \"1\"}\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "def run_conversation():\n",
    "    try:\n",
    "        config = get_graph_config()\n",
    "        \n",
    "        # Initial message\n",
    "        input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "        output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "        for m in output['messages'][-1:]:\n",
    "            m.pretty_print()\n",
    "\n",
    "        # Follow-up messages\n",
    "        messages = [\n",
    "            \"what's my name?\",\n",
    "            \"i like the 49ers!\"\n",
    "        ]\n",
    "        \n",
    "        for message in messages:\n",
    "            input_message = HumanMessage(content=message)\n",
    "            output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "            for m in output['messages'][-1:]:\n",
    "                m.pretty_print()\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in conversation: {str(e)}\")\n",
    "    finally:\n",
    "        # Return the connection to the pool\n",
    "        if config and \"connection\" in config:\n",
    "            db_manager.return_connection(config[\"connection\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
